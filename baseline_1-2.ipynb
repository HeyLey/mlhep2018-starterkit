{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from random import shuffle\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "%matplotlib inline\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "process\n",
    "\n",
    "USE_GPU = True\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' if USE_GPU else ''\n",
    "\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for phase 1 set num_classes=2\n",
    "# for phase 2 set num_classes=4\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train_1-2.hdf5'\n",
    "test_file = 'test_1-2.hdf5'\n",
    "submission_file = 'submission_1-2.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.base import plot_3d, hdf5_to_numpy\n",
    "from tools.tools import calculate_metrics_on_graph, stretch_array, prepare_data_for_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# N -- number of enties to read. Either int or np.inf. In latter case all entries are readed.\n",
    "N = 20\n",
    "X, Y, M, N = hdf5_to_numpy(file=train_file, n=N, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = -1\n",
    "plot_3d(X[k], Y[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce dimensionality of data with clustering\n",
    "\n",
    "We have a lot of reduncancy in the data and only four features: x, y, z, E.  One possible way to deal with it is to preprocess data in the following way:\n",
    "\n",
    "__1)__ Collapse points in small blobs using some criteria. For example, \n",
    "\n",
    "    * blob size < 10;\n",
    "    * max distance in the blob < 1th percentile in full distance matrix.\n",
    "    \n",
    "![](img/clusters.png)\n",
    "    \n",
    "__2)__ For each blob creacte features that represent it geometrical properties: principal axes and 'sizes' of blob in these directions;\n",
    "\n",
    "![](img/pca.png)\n",
    "\n",
    "__3)__ Connect blobs with edges generated by k-nearest neighbour graph algorithm;\n",
    "\n",
    "__4)__ Train MPNN-model on this data.\n",
    "\n",
    "__5)__ Predict labels for blobs.\n",
    "\n",
    "__6)__ Assign blobs labels to all underlying hits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "__max_cl__ -- maximum size of blobs;\n",
    "\n",
    "__fraction__ -- percentile cut level;\n",
    "\n",
    "__n_neighbors__ -- number of neighbours for k-nearest neighbours graph algo(http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html)\n",
    "\n",
    "__out_degree_max__ -- max number of out-edges for each node;\n",
    "\n",
    "__in_degree_max__ -- max number of in-edges for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cl = 10\n",
    "fraction = 1\n",
    "n_neighbors = 3\n",
    "\n",
    "\n",
    "in_degree_max, out_degree_max = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.clustering import Cluster, simple_clusterer, clusterise_data\n",
    "from tools.cluster_aggregation import principal_axes, aggregate_clusters\n",
    "from tools.graph_construction import generate_edges, generate_graph_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_clusters_graph = []\n",
    "for k in tqdm(range(len(X))):\n",
    "    if len(X[k]) == 0:\n",
    "        continue\n",
    "        \n",
    "    # clustering data\n",
    "    X_cluster, Y_cluster, M_cluster = clusterise_data(X[k], Y[k], M[k], verbose=False, \n",
    "                                                      max_cl=max_cl, fraction=fraction)\n",
    "    \n",
    "    # aggregation of cluster statistics\n",
    "    X_cluster_condensed = aggregate_clusters(X_cluster, Y_cluster, M_cluster)\n",
    "\n",
    "    # construction of graph based on aggregated statistics\n",
    "    X_cluster_graph, in_degree_max_local, out_degree_max_local = generate_graph_dataset(X_cluster_condensed=X_cluster_condensed, \n",
    "                                                                                        n_neighbors=n_neighbors)\n",
    "    in_degree_max = max(in_degree_max_local, in_degree_max)\n",
    "    out_degree_max = max(out_degree_max_local, out_degree_max)\n",
    "    \n",
    "    X_clusters_graph.append(X_cluster_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is __in_degree_max__ and __out_degree_max__?\n",
    "\n",
    "Well, when you are working with tensorflow you have to specify shape of your data(at least number of columns).\n",
    "\n",
    "```\n",
    "shape = (number_of_nodes, out_degree/in_degree)\n",
    "```\n",
    "\n",
    "__max_out_degree__ is fixed and equal __n_neighbors__ in our setting, but __in_degree_max__ could be different across different events. \n",
    "\n",
    "To anticipate it we are padding all events with edges to non-existing node. Latter this should be taken into account in the MPNN-algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_max, out_degree_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.tools import calculate_metrics_on_graph, stretch_array\n",
    "\n",
    "for X_cluster_graph in X_clusters_graph:\n",
    "    X_cluster_graph['X_cluster_messages_out'] = stretch_array(X_cluster_graph['X_cluster_messages_out'], \n",
    "                                                              n=out_degree_max, \n",
    "                                                              fill_value=len(X_cluster_graph['X_cluster_edges']))\n",
    "    \n",
    "    X_cluster_graph['X_cluster_messages_in'] = stretch_array(X_cluster_graph['X_cluster_messages_in'], \n",
    "                                                              n=in_degree_max, \n",
    "                                                              fill_value=len(X_cluster_graph['X_cluster_edges']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning model (MPNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, GRUCell, GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster_graph['X_cluster_nodes'].shape, X_cluster_graph['X_cluster_edges'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim_features_nodes = 12\n",
    "ndim_features_edges = 5\n",
    "ndim_message = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nodes = K.placeholder(shape=(None, ndim_features_nodes)) # features of nodes\n",
    "X_edges = K.placeholder(shape=(None, ndim_features_edges)) # features of edges\n",
    "X_labels = K.placeholder(shape=(None, num_classes)) # labels\n",
    "\n",
    "X_nodes_in_out = K.placeholder(shape=(None, 2), dtype=np.int32) # edges\n",
    "X_messages_in = K.placeholder(shape=(None, in_degree_max), dtype=np.int32) # shape = (none, size of neighbourhood)\n",
    "X_messages_out = K.placeholder(shape=(None, out_degree_max), dtype=np.int32) # shape = (none, size of neighbourhood)\n",
    "\n",
    "fake_message_const = K.constant(value=[ndim_message * [-np.inf]]) \n",
    "fake_message_const = K.constant(value=[ndim_message * [0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholders = {\n",
    "    'X_nodes': X_nodes,\n",
    "    'X_edges': X_edges,\n",
    "    'X_labels': X_labels,\n",
    "    'X_nodes_in_out': X_nodes_in_out,\n",
    "    'X_messages_in': X_messages_in,\n",
    "    'X_messages_out': X_messages_out\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_passers = {\n",
    "    0: Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_features_nodes + ndim_features_edges,), activation=relu), \n",
    "                      Dropout(rate=0.2),\n",
    "                      Dense(ndim_message, activation=relu),\n",
    "                      Dropout(rate=0.2),\n",
    "                  ]\n",
    "                 ),\n",
    "    1: Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_features_nodes + ndim_features_edges,), activation=relu), \n",
    "                      Dropout(rate=0.2),\n",
    "                      Dense(ndim_message, activation=relu),\n",
    "                      Dropout(rate=0.2),\n",
    "                  ]\n",
    "                 ),    \n",
    "    2: Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_features_nodes + ndim_features_edges,), activation=relu), \n",
    "                      Dropout(rate=0.2),\n",
    "                      Dense(ndim_message, activation=relu),\n",
    "                      Dropout(rate=0.2),\n",
    "                  ]\n",
    "                 )\n",
    "}\n",
    "\n",
    "\n",
    "#state_updater = tf.contrib.rnn.GRUCell(num_units=ndim_features_nodes, )\n",
    "state_updater = Dense(ndim_features_nodes, input_shape = (2 * ndim_message + ndim_features_nodes,))\n",
    "readout = Dense(num_classes, input_shape=(ndim_features_nodes,), activation=keras.activations.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPNN construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief explanation of MPNN algorithm in a diagram for a following toy graph:\n",
    "\n",
    "![](img/example_graph.png)\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "![](img/mpnn.png)\n",
    "\n",
    "\n",
    "And corresponding code with comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(X_nodes, X_edges, X_nodes_in_out, \n",
    "                  X_messages_in, X_messages_out, message_passers, \n",
    "                  state_updater, readout, ndim_features_nodes, fake_message_const, steps):\n",
    "    # nodes 'talks' to each other several times which is defined by __step__ parameter\n",
    "    for step in range(steps):\n",
    "        # messages from node to node\n",
    "        messages = message_passers[step](\n",
    "            K.concatenate(\n",
    "                [\n",
    "                    K.reshape(K.gather(reference=X_nodes, indices=X_nodes_in_out), \n",
    "                              shape=(-1, 2 * ndim_features_nodes)), \n",
    "                    X_edges\n",
    "                ], axis=1\n",
    "            )\n",
    "        )\n",
    "        # correct dealing with non-existing edge\n",
    "        messages = K.concatenate([messages, fake_message_const], axis=0)\n",
    "        messages = tf.where(tf.is_inf(messages), tf.zeros_like(messages), messages)\n",
    "\n",
    "        # aggregating messages that came into the node\n",
    "        messages_aggregated_in = K.max(K.gather(reference=messages, indices=X_messages_in), axis=1)\n",
    "        # ... and those exiting node\n",
    "        messages_aggregated_out = K.max(K.gather(reference=messages, indices=X_messages_out), axis=1)\n",
    "\n",
    "        # update nodes states based on messages and previous state\n",
    "        X_nodes = state_updater(K.concatenate([messages_aggregated_in, messages_aggregated_out, X_nodes], axis=1))\n",
    "\n",
    "    return readout(X_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.mpnn import build_network, run_train, run_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predictions = build_network(X_nodes=X_nodes, \n",
    "                              X_edges=X_edges, \n",
    "                              X_nodes_in_out=X_nodes_in_out, \n",
    "                              X_messages_in=X_messages_in, \n",
    "                              X_messages_out=X_messages_out, \n",
    "                              message_passers=message_passers, \n",
    "                              state_updater=state_updater, \n",
    "                              readout=readout, \n",
    "                              steps=steps, \n",
    "                              ndim_features_nodes=ndim_features_nodes,\n",
    "                              fake_message_const=fake_message_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tf = tf.reduce_mean(keras.losses.categorical_crossentropy(X_labels, X_predictions))\n",
    "accuracy_tf = tf.reduce_mean(keras.metrics.categorical_accuracy(X_labels, X_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss_tf, var_list=tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "init.run(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(len(X_clusters_graph) * 0.8)\n",
    "\n",
    "shuffle(X_clusters_graph)\n",
    "\n",
    "X_clusters_graph_train = X_clusters_graph[:TRAIN_SIZE]\n",
    "X_clusters_graph_eval = X_clusters_graph[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "roc_aucs = []\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    loss_float = 0\n",
    "    accuracy_float = 0\n",
    "    \n",
    "    losses_epoch = []\n",
    "    accuracies_epoch = []\n",
    "    roc_aucs_epoch = []\n",
    "    for X_cluster_graph in X_clusters_graph_train:\n",
    "        predictions, (loss, accuracy) = run_train(X_cluster_graph=X_cluster_graph,\n",
    "                                   X_predictions=X_predictions,\n",
    "                                   optimizer=optimizer, sess=sess, \n",
    "                                   ndim_features_nodes=ndim_features_nodes, \n",
    "                                   ndim_features_edges=ndim_features_edges, \n",
    "                                   placeholders=placeholders,\n",
    "                                   metrics=[loss_tf, accuracy_tf])\n",
    "        accuracy, roc_auc, predictions_ravel, y_ravel = calculate_metrics_on_graph(X_cluster_graph=X_cluster_graph, predictions=predictions)\n",
    "        losses_epoch.append(loss)\n",
    "        accuracies_epoch.append(accuracy)\n",
    "        roc_aucs_epoch.append(roc_auc)\n",
    "    clear_output()\n",
    "    \n",
    "    roc_aucs.append(np.mean(roc_aucs_epoch))\n",
    "    plt.plot(roc_aucs)\n",
    "    plt.show()\n",
    "\n",
    "    accuracies.append(np.mean(accuracies_epoch))\n",
    "    plt.plot(accuracies)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test = []\n",
    "accuracies_test = []\n",
    "roc_aucs_test = []\n",
    "predictions_ravel_total = [] \n",
    "y_ravel_total =[]\n",
    "\n",
    "for X_cluster_graph in X_clusters_graph_eval:\n",
    "    predictions, (loss, accuracy) = run_test(X_cluster_graph=X_cluster_graph, \n",
    "                                              X_predictions=X_predictions,\n",
    "                                              sess=sess, \n",
    "                                              ndim_features_nodes=ndim_features_nodes, \n",
    "                                              ndim_features_edges=ndim_features_edges, \n",
    "                                              placeholders=placeholders,\n",
    "                                              metrics=[loss_tf, accuracy_tf])\n",
    "    X_cluster_graph['predictions'] = predictions\n",
    "    accuracy, roc_auc, predictions_ravel, y_ravel = calculate_metrics_on_graph(X_cluster_graph=X_cluster_graph, predictions=predictions)\n",
    "    predictions_ravel_total.append(predictions_ravel)\n",
    "    y_ravel_total.append(y_ravel)\n",
    "    losses_test.append(loss)\n",
    "    accuracies_test.append(accuracy)\n",
    "    roc_aucs_test.append(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ravel_total = np.concatenate(predictions_ravel_total)\n",
    "y_ravel_total = np.concatenate(y_ravel_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(np.argmax(y_ravel_total, axis=1), np.argmax(predictions_ravel_total, axis=1))\n",
    "roc_auc = metrics.roc_auc_score(y_ravel_total, predictions_ravel_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test, M_test, N_test = hdf5_to_numpy(file=test_file, n=np.inf, num_classes=num_classes, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clusters_graph_test = []\n",
    "for k in tqdm(range(len(X_test))):\n",
    "    if len(X_test) == 0:\n",
    "        X_cluster_graph = {}\n",
    "        X_cluster_graph['empty'] = True\n",
    "        X_clusters_graph_test.append(X_cluster_graph)\n",
    "        continue\n",
    "        \n",
    "    # clustering\n",
    "    X_cluster, Y_cluster, M_cluster = clusterise_data(X_test[k], Y_test[k], M_test[k], verbose=False, \n",
    "                                                      max_cl=max_cl, fraction=fraction)\n",
    "    \n",
    "    # aggregation\n",
    "    X_cluster_condensed = aggregate_clusters(X_cluster, Y_cluster, M_cluster)\n",
    "\n",
    "    # graph data\n",
    "    X_cluster_graph, in_degree_max_local, out_degree_max_local = generate_graph_dataset(X_cluster_condensed=X_cluster_condensed, \n",
    "                                                                                        n_neighbors=n_neighbors, \n",
    "                                                                                        in_degree_max=in_degree_max, \n",
    "                                                                                        out_degree_max=out_degree_max)\n",
    "\n",
    "    X_cluster_graph['empty'] = False\n",
    "    X_clusters_graph_test.append(X_cluster_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_submission(X_cluster_graph):\n",
    "    submission_answer = np.zeros((192, 192, 192))\n",
    "    shift = 0 if num_classes==4 else 1\n",
    "    for cluster, prediction in zip(X_cluster_graph['clusters'], X_cluster_graph['predictions']):\n",
    "        submission_answer[cluster['M'][:, 0], cluster['M'][:, 1], cluster['M'][:, 2]] = np.argmax(prediction) + shift\n",
    "    X_cluster_graph['submission_answer'] = submission_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_cluster_graph in X_clusters_graph_test:\n",
    "    if not X_cluster_graph['empty']:\n",
    "        predictions, (loss, accuracy) = run_test(X_cluster_graph=X_cluster_graph, \n",
    "                                                  X_predictions=X_predictions,\n",
    "                                                  sess=sess, \n",
    "                                                  ndim_features_nodes=ndim_features_nodes, \n",
    "                                                  ndim_features_edges=ndim_features_edges, \n",
    "                                                  placeholders=placeholders,\n",
    "                                                  metrics=[loss_tf, accuracy_tf])\n",
    "        X_cluster_graph['predictions'] = predictions\n",
    "    \n",
    "    prepare_data_for_submission(X_cluster_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "expectedrows = len(X_test)\n",
    "FILTERS = tables.Filters(complevel=5, complib='zlib', shuffle=True, bitshuffle=False, fletcher32=False, least_significant_digit=None)\n",
    "f_submission = tables.open_file(submission_file, 'w', filters=FILTERS)\n",
    "preds_array = f_submission.create_earray('/', 'pred', tables.UInt32Atom(), (0,192,192,192), expectedrows=expectedrows)\n",
    "\n",
    "for i in tqdm(range(expectedrows)):\n",
    "    preds_array.append(np.expand_dims(X_clusters_graph_test[i]['submission_answer'], axis=0))\n",
    "\n",
    "preds_array.close()\n",
    "f_submission.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
